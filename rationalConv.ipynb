{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686628a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, ReLU, Add, Conv1D, BatchNormalization, LayerNormalization, Layer, Flatten, MaxPooling1D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.image import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed807c",
   "metadata": {},
   "source": [
    "# Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7236b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationalConv(x, filters=16, kernel_size=(3,3), padding=\"same\", start=7, end=3):\n",
    "    \"\"\"\n",
    "    Where ordinary convolutions down-scale by an integer stride, \n",
    "    the rational version up-samples and down-scales an integer stride to \n",
    "    reach a desired rational upscaling.\n",
    "\n",
    "    Example: 7 -> 3 => 7 * bilinear(9/7) = 9, 9 / conv(3) = 3\n",
    "    \"\"\"\n",
    "    size = x.shape\n",
    "    stride = np.ceil(start/end).astype(int)\n",
    "    scaling = (end*stride)/start\n",
    "    \n",
    "    x = resize(x, tf.constant(np.round([size[1]*scaling, size[2]*scaling]), dtype=\"int32\"))\n",
    "    if stride > 1:\n",
    "        x = Conv2D(filters=filters, kernel_size=kernel_size, strides = (stride,stride), padding=padding)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d2ff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 700, 700, 3)]     0         \n",
      "_________________________________________________________________\n",
      "tf.image.resize (TFOpLambda) (None, 900, 900, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 300, 300, 16)      448       \n",
      "=================================================================\n",
      "Total params: 448\n",
      "Trainable params: 448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "input_img = Input(shape=(700,700,3))\n",
    "outputs = rationalConv(input_img, start=7, end=3)\n",
    "model = Model(inputs = [input_img], outputs = [outputs])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029b7f5",
   "metadata": {},
   "source": [
    "# Transposed Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d89f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationalConvTransposed(x, filters=16, kernel_size=(3,3), padding=\"same\", start=3, end=7):\n",
    "    \"\"\"\n",
    "    Where ordinary transposed convolutions up-scale by an integer stride, \n",
    "    the rational version up-scales an integer stride and down-samples to \n",
    "    reach a desired rational upscaling.\n",
    "    \n",
    "    Example: 3 -> 7 => 3 * convTr(3) = 9, 9 * bilinear(7/9) = 7\n",
    "    \"\"\"\n",
    "    size = x.shape\n",
    "    stride = np.ceil(end/start).astype(int)\n",
    "    scaling = end/start\n",
    "    \n",
    "    if stride > 1:\n",
    "        x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides = (stride,stride), padding=padding)(x)\n",
    "    x = resize(x, tf.constant(np.round([size[1]*scaling, size[2]*scaling]), dtype=\"int32\"))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee70245",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cc06ce0d15b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrationalConvTransposed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "input_img = Input(shape=(30,30,3))\n",
    "outputs = rationalConvTransposed(input_img, end=7, start=3)\n",
    "x = Conv2D(8, kernel_size=3, padding=\"same\", activation='relu')(outputs)\n",
    "\n",
    "model = Model(inputs = [input_img], outputs = [x])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df054394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
